[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA: Diabetes Health Indicators",
    "section": "",
    "text": "Introduction\nThe data used in this project comes from the CDC’s 2015 Behavioral Risk Factor Surveillance System. The survey was conducted to collect uniform data on preventative health practices and risk behaviors that are linked to chronic diseases in adults. The diabetes health indicators dataset is a consolidated version of the entire 2015 survey, containing over 250,000 responses and variables related to diabetes, health status, and lifestyle.\nThe primary variable of interest for this project is Diabetes_binary, which classifies respondents into two groups: 0 = no diabetes, 1 = prediabetes or diabetes. Because this survey includes many health indicators that may relate to diabetes risk, this dataset is well-suited for exploring how variables can help predict diabetes status.\nIn this exploratory data analysis, I focus on variables that are known to be related to diabetes, such as BMI, age, general health rating, high blood pressure, high cholesterol, and physical activity. The goal of the EDA is to understand the structure of the data, check for missingness, summarize the distribution of the variables, and visualize how they relate to diabetes status. These insights will help guide my modeling decisions in the next part of this project.\n\n\nData Import and Cleaning\nFirst, I import my dataset. Most of the variables in this dataset are stored as numeric codes even though they represent categories (for example, 0/1 indicators or ordered groups). Before doing any summaries or plotting, I convert these coded variables into factors with meaningful labels so the results are easier to interpret. I also check for missingness in the data, and find no missing values.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read in data\ndiabetes_df &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\n# Convert categorical variables into factors\ndiabetes_df &lt;- diabetes_df |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No Diabetes\", \"Diabetes/Prediabetes\")),\n    HighBP = factor(HighBP, labels = c(\"No High BP\", \"High BP\")),\n    HighChol = factor(HighChol, labels = c(\"No High Chol\", \"High Chol\")),\n    CholCheck = factor(CholCheck, labels = c(\"No Check\", \"Checked\")),\n    Smoker = factor(Smoker, labels = c(\"Non-Smoker\", \"Smoker\")),\n    Stroke = factor(Stroke, labels = c(\"No Stroke\", \"Stroke\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, labels = c(\"No HD/Attack\", \"HD/Attack\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No Activity\", \"Activity\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, labels = c(\"Excellent\",\"Very good\",\"Good\",\"Fair\",\"Poor\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No Difficulty\", \"Difficulty Walking\")),\n    Sex = factor(Sex, labels = c(\"Female\", \"Male\")),\n    Age = factor(Age, levels = 1:13, labels = c(\"18–24\", \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \"80+\")),\n    Education = factor(Education, levels = 1:6, labels = c(\"Kindergarten or less\", \"Grades 1–8\", \"Grades 9–11\", \"High school or GED\", \"Some college/technical school\", \"College graduate\")),\n    Income = factor(Income, levels = 1:8, labels = c(\"&lt;$10,000\", \"$10,000–&lt;$15,000\", \"$15,000–&lt;$20,000\", \"$20,000–&lt;$25,000\", \"$25,000–&lt;$35,000\", \"$35,000–&lt;$50,000\", \"$50,000–&lt;$75,000\", \"$75,000+\"))\n    )\nhead(diabetes_df)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP     HighChol     CholCheck   BMI Smoker     Stroke   \n  &lt;fct&gt;           &lt;fct&gt;      &lt;fct&gt;        &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;    \n1 No Diabetes     High BP    High Chol    Checked      40 Smoker     No Stroke\n2 No Diabetes     No High BP No High Chol No Check     25 Smoker     No Stroke\n3 No Diabetes     High BP    High Chol    Checked      28 Non-Smoker No Stroke\n4 No Diabetes     High BP    No High Chol Checked      27 Non-Smoker No Stroke\n5 No Diabetes     High BP    High Chol    Checked      24 Non-Smoker No Stroke\n6 No Diabetes     High BP    High Chol    Checked      25 Smoker     No Stroke\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;\n\n# Check for NA. No missing values were found.\ncolSums(is.na(diabetes_df))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\n\n\nSummaries and Plots\nBelow I explore the distribution of several variables in the dataset and how they relate to diabetes status. The goal here is to get a sense of the structure of the data, look for patterns, check for imbalance, and see which predictors seem most strongly associated with diabetes.\n\n# Distribution of Diabetes Status\ndiabetes_df |&gt;\n  count(Diabetes_binary) |&gt;\n  ggplot(aes(x = Diabetes_binary, y = n, fill = Diabetes_binary)) +\n  geom_col() +\n  labs(title = \"Distribution of Diabetes Status\", x = \"Diabetes Category\", y = \"Count\") +\n  scale_fill_discrete(name = \"Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe first thing I check is the distribution of the response variable. Most people in the dataset do not have diabetes, with a small portion having diagnosed diabetes or prediabetes. This large imbalance is important because it could potentially affect model performance because models could learn to predict “no diabetes” every time.\n\n# Distribution of Physically Unhealthy Days\nggplot(diabetes_df, aes(x = PhysHlth)) +\n  geom_histogram(bins = 30, fill = \"#7fb3d5\") +\n  labs(title = \"Distribution of Physically Unhealthy Days\", x = \"Number of Physically Unhealthy Days\", y = \"Count\")\n\n\n\n\n\n\n\n\nPhysically unhealthy days has a big spike at zero, meaning a lot of people reported no physical health issues in the past month. However, there’s also a long tail of people who reported many unhealthy days. This variable may help separate people with chronic conditions (including diabetes) from those without.\n\n# Diabetes Rate by Physical Health\ndiabetes_physhlth &lt;- subset(diabetes_df, PhysHlth %in% c(0, 30))\n\ndiabetes_physhlth |&gt;\n  count(PhysHlth, Diabetes_binary) |&gt;\n  group_by(PhysHlth) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = PhysHlth, y = prop, fill = Diabetes_binary)) +\n    geom_col(position = \"fill\") +\n    labs(title = \"Diabetes Status by Physical Health\", x = \"Number of Days Physical Health was Not Good\", y = \"Proportion\") +\n    scale_x_continuous(breaks = c(0, 30)) +\n    scale_fill_discrete(name = \"Diabetes Status\") +\n    theme_minimal()\n\n\n\n\n\n\n\n# Diabetes Rate by Mental Health\ndiabetes_menthlth &lt;- subset(diabetes_df, MentHlth %in% c(0, 30))\n\ndiabetes_menthlth |&gt;\n  count(MentHlth, Diabetes_binary) |&gt;\n  group_by(MentHlth) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = MentHlth, y = prop, fill = Diabetes_binary)) +\n    geom_col(position = \"fill\") +\n    labs(title = \"Diabetes Status by Mental Health\", x = \"Number of Days Mental Health was Not Good\", y = \"Proportion\") +\n    scale_x_continuous(breaks = c(0, 30)) +\n    scale_fill_discrete(name = \"Diabetes Status\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nHere I analyze the relationships between physical and mental health with diabetes. Respondents with 30 days of poor physical health were more likely to have diagnosed diabetes than those with 0 days of poor physical health. The same relationship was noted for mental health, though the difference was less pronounced.\n\n# Distribution of BMI by Diabetes Status\nggplot(diabetes_df, aes(x = BMI, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.4) +\n  labs(title = \"Distribution of Diabetes Status by BMI\",\n       x = \"BMI\", y = \"Density\") +\n  scale_fill_discrete(name = \"Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows that BMI is skewed to the right for all diabetes statuses, but the mean BMI for prediabetic and diabetic respondents is higher than that for non-diabetic respondents. This could indicate a correlation between higher BMI and presence of diabetes.\n\n# Diabetes Rate by Blood Pressure Status\ndiabetes_df |&gt;\n  count(Diabetes_binary, HighBP) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = Diabetes_binary, y = prop, fill = HighBP)) +\n    geom_col(position = \"fill\") +\n    labs(title = \"Blood Pressure Status by Diabetes Status\", x = \"Diabetes Status\", y = \"Proportion\") +\n    scale_fill_discrete(name = \"Blood Pressure Status\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis plot compares blood pressure status across people with diabetes/prediabetes vs no diabetes. People with high blood pressure have a noticeably higher proportion of diabetes and prediabetes compared to people without high blood pressure. This lines up with expectations because high blood pressure and diabetes often occur together. This variable will probably be useful in the model.\n\n# Diabetes Status Across Age Groups\ndiabetes_df |&gt;\n  count(Age, Diabetes_binary) |&gt;\n  ggplot(aes(x = Age, y = n, fill = Diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  labs(title = \"Diabetes Status Across Age Groups\", x = \"Age Ranges\", y = \"Proportion\") +\n  scale_fill_discrete(name = \"Diabetes Status\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThe proportion of diabetes goes up pretty consistently with age. Younger age groups barely show any diabetes cases, while the oldest groups have much higher rates. Because this pattern is so strong, age is likely to be one of the most important predictors.\n\n# Diabetes by General Health Rating\ndiabetes_df |&gt;\n  count(GenHlth, Diabetes_binary) |&gt;\n  ggplot(aes(x = GenHlth, y = n, fill = Diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  labs(title = \"Diabetes Status by General Health Status\", x = \"General Health Status\", y = \"Proportion\") +\n  scale_fill_discrete(name = \"Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot demonstrates that people who rate their general health as “fair” or “poor” have much higher rates of diabetes compared to those who say their health is “excellent” or “very good.” This makes sense and tells me general health might capture a lot of underlying conditions tied to diabetes.\n\n\nModeling Link\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nFor the modeling portion of this project, I build and compare two different models to predict diabetes status using the diabetes health indicators dataset from the CDC’s 2015 Behavioral Risk Factor Surveillance System (BRFSS). The outcome variable is Diabetes_binary, which has two possible categories: 0 = no diabetes and 1 = diabetes/prediabetes. I use the same variables I explored in my EDA including BMI, age, general health, blood pressure, cholesterol, physical activity, and a few others. The goal of this modeling section is to use those variables explored in the EDA to predict diabetes status. I start by splitting the data into a training set (70%) and test set (30%), then fit two different model types (a classification tree and a random forest), tune them using 5-fold cross-validation with log-loss, and compare the two models on the test set. The model with the better performance is the one I will choose as my final model.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.1     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.2.0\n✔ modeldata    1.5.0     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n# Read in data\ndiabetes_df &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE)\n\n# Convert categorical variables into factors\ndiabetes_df &lt;- diabetes_df |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No Diabetes\", \"Diabetes/Prediabetes\")),\n    HighBP = factor(HighBP, labels = c(\"No High BP\", \"High BP\")),\n    HighChol = factor(HighChol, labels = c(\"No High Chol\", \"High Chol\")),\n    CholCheck = factor(CholCheck, labels = c(\"No Check\", \"Checked\")),\n    Smoker = factor(Smoker, labels = c(\"Non-Smoker\", \"Smoker\")),\n    Stroke = factor(Stroke, labels = c(\"No Stroke\", \"Stroke\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, labels = c(\"No HD/Attack\", \"HD/Attack\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No Activity\", \"Activity\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, labels = c(\"Excellent\",\"Very good\",\"Good\",\"Fair\",\"Poor\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No Difficulty\", \"Difficulty Walking\")),\n    Sex = factor(Sex, labels = c(\"Female\", \"Male\")),\n    Age = factor(Age, levels = 1:13, labels = c(\"18–24\", \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \"80+\")),\n    Education = factor(Education, levels = 1:6, labels = c(\"Kindergarten or less\", \"Grades 1–8\", \"Grades 9–11\", \"High school or GED\", \"Some college/technical school\", \"College graduate\")),\n    Income = factor(Income, levels = 1:8, labels = c(\"&lt;$10,000\", \"$10,000–&lt;$15,000\", \"$15,000–&lt;$20,000\", \"$20,000–&lt;$25,000\", \"$25,000–&lt;$35,000\", \"$35,000–&lt;$50,000\", \"$50,000–&lt;$75,000\", \"$75,000+\"))\n    )\n\n# Check diabetes distribution\ndiabetes_df |&gt;\n  count(Diabetes_binary) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary           n  prop\n  &lt;fct&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 No Diabetes          218334 0.861\n2 Diabetes/Prediabetes  35346 0.139\n\n\n\n\nSplit the Data\nNow, I’m going to split the data into a training set and a testing set.\n\n# Set seed for reproducability\nset.seed(123)\n\n# Split the data into 70%/30%\ndiabetes_split &lt;- initial_split(diabetes_df, prop = 0.7, strata = Diabetes_binary)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test  &lt;- testing(diabetes_split)\n\n# 5-fold cross-validation on training data\nset.seed(123)\ndiabetes_folds &lt;- vfold_cv(diabetes_train, v = 5, strata = Diabetes_binary)\n# Check distribution for training set\ndiabetes_train |&gt;\ncount(Diabetes_binary) |&gt;\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary           n  prop\n  &lt;fct&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 No Diabetes          152833 0.861\n2 Diabetes/Prediabetes  24742 0.139\n\n# Check distribution for testing set\ndiabetes_test |&gt;\ncount(Diabetes_binary) |&gt;\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary          n  prop\n  &lt;fct&gt;                &lt;int&gt; &lt;dbl&gt;\n1 No Diabetes          65501 0.861\n2 Diabetes/Prediabetes 10604 0.139\n\n\n\n\nClassification Tree\nA classification tree works by repeatedly splitting the data into smaller groups based on the predictor variables. Each split is chosen to separate the diabetes categories as well as possible. Trees are very interpretable, but a single tree can overfit if it’s allowed to grow too big. To prevent that, I tune the cost_complexity parameter, which controls how much the model penalizes overly complicated trees. I use 5-fold cross-validation with log-loss as the tuning metric and choose the value of cost complexity that minimizes log-loss. After selecting the best value, I fit the final tree model on the training data and evaluate it on the test set.\n\n# Tree model specification\ntree_spec &lt;- decision_tree(\nmode = \"classification\",\ncost_complexity = tune()\n) |&gt;\nset_engine(\"rpart\")\n\n# Workflow: recipe + model\ntree_wf &lt;- workflow() |&gt;\nadd_model(tree_spec) |&gt;\nadd_recipe(\nrecipe(Diabetes_binary ~ BMI + Age + GenHlth + HighBP + HighChol +\nPhysActivity + DiffWalk + Sex + Education + Income,\ndata = diabetes_train)\n)\n\n# Grid of cost_complexity values\ntree_grid &lt;- grid_regular(cost_complexity(), levels = 10)\n\n# Metrics\ntree_metrics &lt;- metric_set(mn_log_loss)\n\n# Tune with CV\nset.seed(123)\ntree_res &lt;- tune_grid(\ntree_wf,\nresamples = diabetes_folds,\ngrid = tree_grid,\nmetrics = tree_metrics\n)\n\ncollect_metrics(tree_res)\n\n# A tibble: 10 × 7\n   cost_complexity .metric     .estimator  mean     n    std_err .config        \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;          \n 1    0.0000000001 mn_log_loss binary     0.379     5 0.00212    Preprocessor1_…\n 2    0.000000001  mn_log_loss binary     0.379     5 0.00212    Preprocessor1_…\n 3    0.00000001   mn_log_loss binary     0.379     5 0.00212    Preprocessor1_…\n 4    0.0000001    mn_log_loss binary     0.379     5 0.00212    Preprocessor1_…\n 5    0.000001     mn_log_loss binary     0.379     5 0.00212    Preprocessor1_…\n 6    0.00001      mn_log_loss binary     0.377     5 0.00187    Preprocessor1_…\n 7    0.0001       mn_log_loss binary     0.342     5 0.00136    Preprocessor1_…\n 8    0.001        mn_log_loss binary     0.353     5 0.000687   Preprocessor1_…\n 9    0.01         mn_log_loss binary     0.404     5 0.00000973 Preprocessor1_…\n10    0.1          mn_log_loss binary     0.404     5 0.00000973 Preprocessor1_…\n\n\n\n# Select best model for classification tree\nbest_tree &lt;- select_best(tree_res, metric = \"mn_log_loss\")\nbest_tree\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1          0.0001 Preprocessor1_Model07\n\n# Finalize workflow\ntree_final_wf &lt;- finalize_workflow(tree_wf, best_tree)\n\n# Evaluate on test set\ntree_last_fit &lt;- last_fit(tree_final_wf, diabetes_split)\n\ntree_test_metrics &lt;- collect_metrics(tree_last_fit)\ntree_test_metrics\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.862 Preprocessor1_Model1\n2 roc_auc     binary         0.786 Preprocessor1_Model1\n3 brier_class binary         0.103 Preprocessor1_Model1\n\n\nBased on 5-fold cross-validation, the cost complexity value that minimized log-loss was 0.0001. This means the model performed best with a moderate amount of pruning, so I selected this value for the final tree and evaluated it on the test set. The finalized tree reached an accuracy of about 0.862, a ROC AUC of about 0.745, and a Brier score of about 0.104 on the test data. These metrics show that the tree does reasonably well at predicting diabetes status but there is still plenty of room for improvement. They also give me a baseline to compare against the random forest model which will help me decide which model performs better overall.\n\n\nRandom Forest\nA random forest is a collection of decision trees that all make their own predictions, and then the forest averages them together. Random forests usually perform better than a single tree because each tree sees a slightly different bootstrap sample of the data and only looks at a random subset of predictors at each split. This reduces variance and often improves predictive performance. The main parameter I tune here is mtry, which controls how many predictors each tree is allowed to look at when it chooses a split. I use 5-fold cross-validation and log-loss to find the best value of mtry. After finding the best value of mtry, I fit the final random forest model on the training data and evaluate it on the test set.\n\n# Random forest specification\nrf_spec &lt;- rand_forest(\nmode = \"classification\",\nmtry = tune(),\ntrees = 100\n) |&gt;\nset_engine(\"ranger\")\n\n# Workflow\nrf_wf &lt;- workflow() |&gt;\nadd_model(rf_spec) |&gt;\nadd_recipe(\nrecipe(Diabetes_binary ~ BMI + Age + GenHlth + HighBP + HighChol +\nPhysActivity + DiffWalk + Sex + Education + Income,\ndata = diabetes_train)\n)\n\n# Grid for mtry\nrf_grid &lt;- tibble(mtry = c(2, 4, 6, 8))\n\nrf_metrics &lt;- metric_set(mn_log_loss)\n\nset.seed(123)\nrf_res &lt;- tune_grid(\nrf_wf,\nresamples = diabetes_folds,\ngrid = rf_grid,\nmetrics = rf_metrics\n)\n\ncollect_metrics(rf_res)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     2 mn_log_loss binary     0.319     5 0.000515 Preprocessor1_Model1\n2     4 mn_log_loss binary     0.330     5 0.000630 Preprocessor1_Model2\n3     6 mn_log_loss binary     0.356     5 0.00184  Preprocessor1_Model3\n4     8 mn_log_loss binary     0.394     5 0.00127  Preprocessor1_Model4\n\n\n\n# Select best model for random forest\nbest_rf &lt;- select_best(rf_res, metric = \"mn_log_loss\")\nbest_rf\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;dbl&gt; &lt;chr&gt;               \n1     2 Preprocessor1_Model1\n\n# Finalize workflow and evaluate on test set\nrf_final_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\nrf_last_fit &lt;- last_fit(rf_final_wf, diabetes_split)\n\nrf_test_metrics &lt;- collect_metrics(rf_last_fit)\nrf_test_metrics\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.866  Preprocessor1_Model1\n2 roc_auc     binary        0.820  Preprocessor1_Model1\n3 brier_class binary        0.0985 Preprocessor1_Model1\n\n\nUsing 5-fold cross-validation, the value of mtry that minimized log-loss was 2. This means the model performed best when each tree only considered a small subset of predictors at each split, which helps reduce overfitting and increases the variety among the trees. After selecting this tuning value, I fit the final random forest on the training data and evaluated it on the test set. The model achieved an accuracy of about 0.865, a ROC AUC of about 0.820, and a Brier score of about 0.100. These results show that the random forest provides reasonably strong predictions for the diabetes categories, both in terms of classification performance and probability accuracy.\n\n\nFinal Model Selection\n\n# Compute test-set log-loss for the tree\ntree_last_fit_log &lt;- last_fit(\n  tree_final_wf,\n  diabetes_split,\n  metrics = metric_set(mn_log_loss)\n)\n\n# Compute test-set log-loss for the random forest\nrf_last_fit_log &lt;- last_fit(\n  rf_final_wf,\n  diabetes_split,\n  metrics = metric_set(mn_log_loss)\n)\n\ncollect_metrics(tree_last_fit_log)\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.340 Preprocessor1_Model1\n\ncollect_metrics(rf_last_fit_log)\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.319 Preprocessor1_Model1\n\n\nTo decide which model performed better, I compared the test-set log-loss for the tuned classification tree and the tuned random forest. The random forest had a lower log-loss of 0.320 compared to the classification tree 0.352, which means it produced better probability predictions on new data. Since the random forest performed better on this metric, I selected it as my final model."
  }
]